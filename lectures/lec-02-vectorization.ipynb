{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 02: Vectorization\n",
    "\n",
    "**NB. Go to section, or you will fail the course.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## TF-IDF weighting for word counts\n",
    "\n",
    "* Why do we sometimes remove stopwords from our features?\n",
    "    * High-frequency words shared by many documents don't tell us (in many, but not all, cases) much about the similarities or differences between documents\n",
    "* But stopword lists are binary: a word is either a stopword (hence, removed) or it isn't\n",
    "* Can we define a continuous adjustment for \"stoppiness\" that we apply to *every* word, depending on how widely used it is?\n",
    "* One approach is \"term frequency-inverse document frequency\" (TFIDF) weighting. \n",
    "    * You can think of this as multiplying the count of each term in a document by the inverse of the fraction of all documents in which that word occurs (hence \"term frequency [multiplied by] inverse document frequency\"). It's a bit more complicated than that (see below), but that's the idea. This upweights words that occur in relatively few documents.\n",
    "    * The count of a word that occurs in every document would be multiplied by one, hence get no boost in each document. A word that occurs in just one document in a corpus of 100 documents would be multiplied by 100 in the one document that contains it.\n",
    "* There are several tweaks to TFIDF to smooth it out and to modulate the boost it provides. `scikit-learn`'s `TfidfVectorizer` applies the reweighting:\n",
    "\n",
    "$$\\text{idf}(t) = \\ln\\frac{1+n}{1 + \\text{df}(t)} + 1$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $t$ is the term in question\n",
    "* $\\text{idf}(t)$ is the inverse document weight to be applied to the count of term $t$\n",
    "* $n$ is the number of documents in the corpus\n",
    "* $\\text{df}(t)$ is the number of documents in the corpus that contain term $t$\n",
    "\n",
    "A toy example: Consider two documents:\n",
    "\n",
    "* Document 1: `\"cat dog\"`\n",
    "* Document 2: `\"dog dog\"`\n",
    "\n",
    "`cat` occurs in just one document; `dog` occurs in both documents. So we want (and expect) to upweight the count of `cat` in document 1.\n",
    "\n",
    "Calculate the `idf` weight for `cat` in document 1:\n",
    "\n",
    "* $n = 2$\n",
    "* $\\text{df}(\\text{`cat'}) = 1$\n",
    "\n",
    "$$\\text{idf}(\\text{`cat'}) = \\ln\\frac{1 + 2}{1 + 1} + 1 = \\ln\\frac{3}{2} + 1 = 1.405$$\n",
    "\n",
    "And for `dog` in document 1:\n",
    "\n",
    "* $n = 2$\n",
    "* $\\text{df}(\\text{`dog'}) = 2$\n",
    "\n",
    "$$\\text{idf}(\\text{`dog'}) = \\ln\\frac{1 + 2}{1 + 2} + 1 = \\ln\\frac{3}{3} + 1 = 1.0$$\n",
    "\n",
    "So, `cat` will be upweighted relative to `dog`, because it is the less widely used word across documents in the corpus.\n",
    "\n",
    "Our non-normalized but IDF-weighted feature matrix will look like this:\n",
    "\n",
    "```\n",
    "cat  dog\n",
    "1.4  1.0\n",
    "0    2.0\n",
    "```\n",
    "\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Corpus ###\n",
      " ['cat dog', 'dog dog'] \n",
      "\n",
      "### Feature matrix *without* IDF weighting ###\n",
      "Feature names: ['cat' 'dog']\n",
      "[[0.70710678 0.70710678]\n",
      " [0.         1.        ]]\n",
      "\n",
      "### Feature matrix *with* IDF weighting ###\n",
      "Feature names: ['cat' 'dog']\n",
      "[[0.81480247 0.57973867]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"cat dog\",\n",
    "    \"dog dog\",\n",
    "]\n",
    "print(\"### Corpus ###\\n\", corpus, \"\\n\")\n",
    "\n",
    "# without IDF weighting (note l2 norm)\n",
    "vectorizer_no_idf = TfidfVectorizer(\n",
    "    use_idf=False\n",
    ")\n",
    "features_no_idf = vectorizer_no_idf.fit_transform(corpus)\n",
    "print(\"### Feature matrix *without* IDF weighting ###\")\n",
    "print(\"Feature names:\", vectorizer_no_idf.get_feature_names_out())\n",
    "print(features_no_idf.toarray())\n",
    "\n",
    "# with IDF weighting\n",
    "vectorizer_with_idf = TfidfVectorizer(\n",
    "    use_idf=True,\n",
    ")\n",
    "features_with_idf = vectorizer_with_idf.fit_transform(corpus)\n",
    "print(\"\\n### Feature matrix *with* IDF weighting ###\")\n",
    "print(\"Feature names:\", vectorizer_with_idf.get_feature_names_out())\n",
    "print(features_with_idf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, in document 1, `cat` has been up-weighted while `dog` has been downweighted. There's no change in document 2 because that document has only a single word type and `TfidfVectorizer`'s `l2` norm enforces total feature weights whose squares sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2-normed, hand calculated, IDF weighted features for document 1\n",
      "[0.81471182 0.57986606]\n"
     ]
    }
   ],
   "source": [
    "# Check our hand calculation against code version\n",
    "import numpy as np\n",
    "vec = np.array([1.405, 1.0])     # hand calculation\n",
    "l2_vec = vec/np.linalg.norm(vec) # calculate l2 normalized version\n",
    "print(\"l2-normed, hand calculated, IDF weighted features for document 1\")\n",
    "print(l2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# do our two versions match?\n",
    "assert np.allclose(l2_vec, features_with_idf[0,].toarray(), atol=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom functions\n",
    "\n",
    "`Sklearn` vectorizers have [settings for common options](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). But sometimes, you need to plug in your own code for a special case. For example, what about Chinese-language input? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['因为无法解决与中国合作伙伴的纠纷' '因受新型冠状病毒危机对足球和其他体育赛事的持续影响' '已终止了其最赚钱的海外转播合同'\n",
      " '早已面临越来越多亏损的英格兰超级足球联赛周四宣布']\n",
      "Values: [[0.5 0.5 0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Strings\n",
    "zh = '因受新型冠状病毒危机对足球和其他体育赛事的持续影响，早已面临越来越多亏损的英格兰超级足球联赛周四宣布，因为无法解决与中国合作伙伴的纠纷，已终止了其最赚钱的海外转播合同。'\n",
    "en = 'The English Premier League, already facing mounting losses because of the continued impact of the coronavirus crisis on soccer and other sporting events, announced on Thursday that it had canceled its most lucrative overseas broadcast contract after it was unable to resolve a dispute with its Chinese partner.'\n",
    "\n",
    "vectorizer_default = TfidfVectorizer(input='content')\n",
    "chinese_features = vectorizer_default.fit_transform([zh])\n",
    "print(\"Feature names:\", vectorizer_default.get_feature_names_out())\n",
    "print(\"Values:\", chinese_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/nk/n3rklsf51vbb6gxd3z9znxr00000gp/T/jieba.cache\n",
      "Loading model cost 0.281 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['。' '与' '中国' '了' '亏损' '体育赛事' '其' '其他' '冠状病毒' '危机' '合作伙伴' '合同' '周四' '和'\n",
      " '因为' '因受' '多' '宣布' '对' '已' '影响' '持续' '新型' '无法' '早已' '最' '海外' '的' '纠纷'\n",
      " '终止' '英格兰' '解决' '赚钱' '超级' '越来越' '足球' '足球联赛' '转播' '面临' '，']\n",
      "Values: [[0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.50395263 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.12598816 0.12598816 0.12598816\n",
      "  0.12598816 0.12598816 0.12598816 0.37796447]]\n"
     ]
    }
   ],
   "source": [
    "# Custom tokenizer\n",
    "import jieba\n",
    "\n",
    "def chinese_tokenizer(x):\n",
    "    '''Tokenize a Chinese-language string'''\n",
    "    return jieba.lcut(x)\n",
    "    \n",
    "vectorizer_chinese = TfidfVectorizer(\n",
    "    input='content',\n",
    "    tokenizer=chinese_tokenizer,\n",
    "    token_pattern=None\n",
    ")\n",
    "chinese_features = vectorizer_chinese.fit_transform([zh])\n",
    "print(\"Feature names:\", vectorizer_chinese.get_feature_names_out())\n",
    "print(\"Values:\", chinese_features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese vector length: 1.0\n"
     ]
    }
   ],
   "source": [
    "# check norm = l2\n",
    "# cf. https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html\n",
    "\n",
    "print(\"Chinese vector length:\", np.linalg.norm(chinese_features.toarray().T, ord=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
