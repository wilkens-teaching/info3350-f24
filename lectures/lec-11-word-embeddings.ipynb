{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 11: Static word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words as vectors\n",
    "\n",
    "**Big picture: static word embeddings replace each word in our corpus with a vector** (typically of dimensionality between 100 and 1,000) **that captures the range of meanings of that word. We can use these vectors in place of token counts as the input features for any downstream NLP or text analysis task.**\n",
    "\n",
    "* We've previously represented **documents** as vectors\n",
    "    * Usually, vectors of word counts\n",
    "    * Or, with topic models, as vectors of topics\n",
    "* Limits of words, often discussed\n",
    "    * In brief: some words are more alike than others, but word counts don't reflect this fact\n",
    "    * Topic models helped to address this problem\n",
    "    * Today, a different approach: **word embeddings**\n",
    "* We might like to have a vector for each **word**, which we could in turn use to build our document vectors\n",
    "    * This will be an *unsupervised* task. We want to produce vectors that place similar words close together in vector space, but we don't want to use human-labled data to do so. We want instead to observe patterns of use in a large corpus.\n",
    "* One approach (not pursued here): Truncated SVD on sliding windows over words\n",
    "    * Group all the resulting bags of words by their 'center' word\n",
    "    * Reduce resulting \"doc\"-term matrix for each word via truncated SVD\n",
    "        * Like latent semantic analysis, but oriented around words rather than documents\n",
    "    * Relatively fast to train and decent performance\n",
    "    * Limited by linear relationships and lack of semantic connections\n",
    "* Language models\n",
    "    * Large neural language models achieve state-of-the-art performance on many NLP tasks\n",
    "    * These models generally seek to predict a word (or other element of a text sequence) on the basis of the words around it\n",
    "    * General architecture is:\n",
    "        * Embedding layer. Counts of words transformed into contextual vectors. Initially, weights are random. In the case of word embeddings, we want to learn the proper weights as our output objective.\n",
    "        * Intermediate (hidden) layer. Neural layers that can learn non-linear relationships between words and embeddings.\n",
    "        * Output layer. Optimize an objective function (for example, highest probability of next word, given an input sequence).\n",
    "    * `word2vec` (2013) showed that we can do without the intermediate layer if we just want to produce word embeddings\n",
    "        * This simplifies the computation, so we can use *much* more input text\n",
    "* CBOW vs. skip-gram\n",
    "    * `word2vec` can use two different models: Continuous bag of words (CBOW) and skip-gram\n",
    "    * CBOW is more \"traditional.\" Objective is to predict a word given the *n* words before and after it.\n",
    "    * Skip-gram \"flips\" the prediction task. Try to predict the *n* contextual words, given a seed word.\n",
    "    * CBOW is faster to train and better captures *syntax* (plurals are closer to other plurals in vector space)\n",
    "    * Skip-gram is slower to train and better captures *semantic* relationships ('king' moves  closer to 'prince' than to 'kings')\n",
    "* Training is slow either way, so using pretrained embeddings is standard practice, but ...\n",
    "    * Embedded relationships and biases of training set\n",
    "    * Change over time\n",
    "    * Polysemy remains within the embedding\n",
    "        * \"Class\" vector has components of education and of social status, for example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's *use* some embeddings\n",
    "\n",
    "We're not going to train them, because that's slow and requires lots of corpus text. In a future lecture, we'll discuss transfer learning. Transfer learning allows us to use a modest amount of new training data to update a larger, pretrained model.\n",
    "\n",
    "### We need a `spaCy` medium or large model\n",
    "\n",
    "The small models that we used in the previous lecture do not include vectors. If you haven't already installed the large model (which includes `GloVe` embeddings, an alternative to `word2vec`), run the code below in a terminal to download and install it.\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up spaCy NLP object and load model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\") # Note '_lg' = large model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words in vector space\n",
    "\n",
    "Take a quick look at some vectors in embedding space. We begin by loading (a sample of) the embedding space, so that we can learn its PCA representation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;PCA<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>PCA()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "PCA()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load embeddings from model, reduce via PCA, plot some example words\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from   sklearn.decomposition import PCA\n",
    "\n",
    "# sample embedding space\n",
    "pct_to_sample = 100 # how much of the vector space to sample?\n",
    "tokens_with_vector = []\n",
    "for string in nlp.vocab.strings: # not all tokens have vectors; get those that do\n",
    "    if nlp.vocab[string].has_vector:\n",
    "        tokens_with_vector.append(string)\n",
    "tokens_in_sample = np.random.choice(tokens_with_vector, int(len(tokens_with_vector) * pct_to_sample/100), replace=False)\n",
    "\n",
    "# get embeddings\n",
    "   # pre-allocate numpy array to hold embeddings\n",
    "embedding_space = np.zeros((len(tokens_in_sample), nlp.vocab.vectors_length), dtype='f')\n",
    "for i, token in enumerate(tokens_in_sample):\n",
    "    embedding_space[i] = nlp.vocab[token].vector\n",
    "\n",
    "# reduce dimensions for plotting\n",
    "pca = PCA()\n",
    "pca.fit(embedding_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "... then we get the PCA reductions of a few words to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAGdCAYAAADT1TPdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsV0lEQVR4nO3deXQUZb7/8U8nIU3WhkSSEEwwDAgEEJGIl0WJoyCLCMdxJWwy12UuyOYCiCiKEGBkUy4oOCfEHxPgOiOIuCAqRJDFQETWYZEIDItxm24WEyBdvz8YemgTBKSbfpJ+v86pc6innqr6Vk3s/kzVU9U2y7IsAQAAGCok0AUAAAD8GsIKAAAwGmEFAAAYjbACAACMRlgBAABGI6wAAACjEVYAAIDRCCsAAMBoYYEu4JfcbrcOHTqkmJgY2Wy2QJcDAAAugmVZOnr0qJKTkxUS4ttrIcaFlUOHDiklJSXQZQAAgN/gwIEDuvrqq326TePCSkxMjKQzBxsbGxvgagAAwMVwuVxKSUnxfI/7knFh5eytn9jYWMIKAACVjD+GcDDAFgAAGI2wAgAAjEZYAQAARiOsAEFkzJgxSkxMlM1m0+LFi8/bBgAmsVmWZQW6iHO5XC45HA45nU4G2AI+tGPHDqWnp2vRokX6r//6L9WsWVN79+4t12a32wNdKoBKyJ/f38Y9DQTAP77++mtJUvfu3T2j9StqAwDTcBsIqETcbrcmTpyo+vXry263KzU1VePGjZMkbdmyRb///e8VERGh+Ph4PfLIIzp27JikM7d6unXrJkkKCQmRzWarsO2snJwcNW7cWNWrV1ejRo00c+ZMrzoOHjyo+++/XzVr1lR8fLy6d++ub7755gqcAQDBiLACVCIjR47UxIkTNXr0aG3fvl15eXlKTEzUiRMn1KlTJ9WsWVMFBQV666239PHHH2vgwIGSpCeffFI5OTmSpMOHD+vw4cMVtknSnDlzNGrUKI0bN047duzQ+PHjNXr0aOXm5kqSTpw4oVtvvVXR0dH67LPPtHr1akVHR6tTp046efJkAM4KgCrPukT5+fnWnXfeadWuXduSZC1atOi8fR955BFLkjV16tSL3r7T6bQkWU6n81JLA6oc9+nT1rF1661/vbvUOvzJp5bdbrfmzJlTrt/s2bOtmjVrWseOHfO0vffee1ZISIh15MgRy7Isa9GiRdYv/5OvqC0lJcXKy8vzahs7dqzVunVry7Is6y9/+YvVsGFDy+12e5aXlpZaERER1rJlyy7vgAFUWv78/r7kMSvHjx9X8+bN9dBDD+kPf/jDefstXrxY69evV3Jy8m+MUUBwc330kb4dn63TR45Ikjb//LNKS0vVKjy8XN8dO3aoefPmioqK8rS1bdtWbrdbO3fuVGJi4kXt87vvvtOBAwf0xz/+UQ8//LCn/fTp03I4HJKkjRs3as+ePeVeqV1SUuIZAwMAvnTJYaVz587q3Lnzr/Y5ePCgBg4cqGXLlqlr166/uTggWLk++kgHBw+RznlYr3rImTEl377wolxJSYrt2NGzzLKs8w6QvZSBs263W9KZW0E33XST17LQ0FBPn5YtW+qvf/1rufVr1ap10fsCgIvl86eB3G63evfuraeeekpNmjTx9eaBKs8qK9O347O9gook1a0Wruo2m9adOK5rxmcr5rbbZPt3gEhPT1dubq6OHz/uubry+eefKyQkRNdee+1F7zsxMVF16tTR3r17lZWVVWGfG264QQsXLlRCQgKvFwBwRfh8gO3EiRMVFhamQYMGXVT/0tJSuVwurwkIZic2bPTc+jmXPSREf4yL18vFxfr7zp3auvgdrVu3Tn/5y1+UlZWl6tWrq2/fvtq6datWrFihxx9/XL17977oW0BnjRkzRtnZ2Zo+fbp27dqlLVu2KCcnR1OmTJEkZWVl6aqrrlL37t21atUqFRUVKT8/X4MHD9Y///lPn5wDADiXT8PKxo0bNX36dM2dO/eiLz1nZ2fL4XB4ppSUFF+WBFQ6p7/77rzL/hQfr35xcXr1++/U8oH7df/996u4uFiRkZFatmyZfvzxR91444265557dNttt2nGjBmXvP///u//1htvvKG5c+eqWbNmat++vebOnau0tDRJUmRkpD777DOlpqbq7rvvVuPGjdW/f3/9/PPPXGkB4BeX9QZbm82mRYsWqUePHpKkadOmadiwYQoJ+U8GKisrU0hIiFJSUip8D0NpaalKS0s98y6XSykpKbzBFkHr+PovtL9v3wv2S83NVdRNra5ARQBwYZXmDba9e/fW7bff7tV2xx13qHfv3nrooYcqXMdut/N6b+AckRktFZaUpNPffltu3IokyWZTWGKiIjNaXvniACAALjmsHDt2THv27PHMFxUVadOmTYqLi1Nqaqri4+O9+lerVk1JSUlq2LDh5VcLBAFbaKgSnxl55mkgm807sPz79mriMyM9g2sBoKq75DErGzZsUIsWLdSiRQtJ0rBhw9SiRQs999xzPi8OCFaxHTuqzvRpCvvF4NiwxETVmT7N67FlAKjq+NVlwGBWWdmZp4O++05htWopMqMlV1QAGKnSjFkB4Fu20FAG0QIIevyQIQAAMBphBQAAGI2wAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwGmEFAAAYjbACAACMRlgBAABGI6wAAACjEVYAAIDRCCsAAMBohBUAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYDTCCgAAMBphBQAAGI2wAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwGmEFAAAYjbACAACMRlgBAABGI6wAAACjEVYAAIDRCCsAAMBohBUAAGA0wgoAADAaYQUAABiNsAIAAIx2yWHls88+U7du3ZScnCybzabFixd7lp06dUrDhw9Xs2bNFBUVpeTkZPXp00eHDh3yZc0AACCIXHJYOX78uJo3b64ZM2aUW3bixAkVFhZq9OjRKiws1Ntvv61du3bprrvu8kmxAAAg+Ngsy7J+88o2mxYtWqQePXqct09BQYFatWqlffv2KTU19YLbdLlccjgccjqdio2N/a2lAQCAK8if399hPt1aBZxOp2w2m2rUqFHh8tLSUpWWlnrmXS6Xv0sCAACViF8H2JaUlGjEiBHq2bPneVNWdna2HA6HZ0pJSfFnSQAAoJLxW1g5deqUHnjgAbndbs2cOfO8/UaOHCmn0+mZDhw44K+SAABAJeSX20CnTp3Sfffdp6KiIn366ae/eu/KbrfLbrf7owwAAFAF+DysnA0qu3fv1ooVKxQfH+/rXQAAgCByyWHl2LFj2rNnj2e+qKhImzZtUlxcnJKTk3XPPfeosLBQS5cuVVlZmY4cOSJJiouLU3h4uO8qBwAAQeGSH11euXKlbr311nLtffv21ZgxY5SWllbheitWrFBmZuYFt8+jywAAVD5GPbqcmZmpX8s3l/HaFgAAgHL4bSAAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYDTCCgAAMBphBQAAGI2wAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwGmEFAAAYjbACAACMRlgBAABGI6wAAACjEVYAAIDRCCsAAMBohBUAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYDTCCgAAMBphBQAAGI2wAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwGmEFAAAYjbACAACMdslh5bPPPlO3bt2UnJwsm82mxYsXey23LEtjxoxRcnKyIiIilJmZqW3btvmqXgAAEGQuOawcP35czZs314wZMypcPmnSJE2ZMkUzZsxQQUGBkpKS1KFDBx09evSyiwUAAMEn7FJX6Ny5szp37lzhMsuyNG3aNI0aNUp33323JCk3N1eJiYnKy8vTo48+ennVAgCAoOPTMStFRUU6cuSIOnbs6Gmz2+1q37691qxZU+E6paWlcrlcXhMAAMBZPg0rR44ckSQlJiZ6tScmJnqW/VJ2drYcDodnSklJ8WVJAACgkvPL00A2m81r3rKscm1njRw5Uk6n0zMdOHDAHyUBAIBK6pLHrPyapKQkSWeusNSuXdvTXlxcXO5qy1l2u112u92XZQAAgCrEp1dW0tLSlJSUpOXLl3vaTp48qfz8fLVp08aXuwIAAEHikq+sHDt2THv27PHMFxUVadOmTYqLi1NqaqqGDBmi8ePHq0GDBmrQoIHGjx+vyMhI9ezZ06eFAwCA4HDJYWXDhg269dZbPfPDhg2TJPXt21dz587V008/rZ9//ln/8z//o59++kk33XSTPvroI8XExPiuagAAEDRslmVZgS7iXC6XSw6HQ06nU7GxsYEuBwAAXAR/fn/z20AAAMBohBUAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYDTCCgAAMBphBQAAGI2wAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwGmEFAAAYjbACAACMRlgBAABGI6wAAACjEVYAAIDRCCsAAMBohBUAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYDTCCgAAMBphBQAAGI2wAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwms/DyunTp/Xss88qLS1NERERqlevnl588UW53W5f7woAAASBMF9vcOLEiXrttdeUm5urJk2aaMOGDXrooYfkcDg0ePBgX+8OAABUcT4PK2vXrlX37t3VtWtXSdI111yj+fPna8OGDb7eFQAACAI+vw3Url07ffLJJ9q1a5ck6auvvtLq1avVpUuXCvuXlpbK5XJ5TQAAAGf5/MrK8OHD5XQ61ahRI4WGhqqsrEzjxo3Tgw8+WGH/7OxsvfDCC74uAwAAVBE+v7KycOFCzZs3T3l5eSosLFRubq5efvll5ebmVth/5MiRcjqdnunAgQO+LgkAAFRiNsuyLF9uMCUlRSNGjNCAAQM8bS+99JLmzZunf/zjHxdc3+VyyeFwyOl0KjY21pelAQAAP/Hn97fPr6ycOHFCISHemw0NDeXRZQAA8Jv4fMxKt27dNG7cOKWmpqpJkyb68ssvNWXKFPXv39/XuwIAAEHA57eBjh49qtGjR2vRokUqLi5WcnKyHnzwQT333HMKDw+/4PrcBgIAoPLx5/e3z8PK5SKsAABQ+VSqMSsAAAC+RFgBAABGI6wAAACjEVYAAIDRCCsAAMBohBUAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYLSgDCuZmZkaMmRIhcv69eunHj16XNF6AADA+QVlWPk106dP19y5cz3zFQWblStXymaz6V//+tcVrQ0AgGAUFugCTONwOAJdAgAAOAdXViR9+OGHcjgcevPNN71uA/Xr10/5+fmaPn26bDabbDabvvnmG916662SpJo1a8pms6lfv36SJMuyNGnSJNWrV08RERFq3ry5/va3v3n2c/aKzCeffKKMjAxFRkaqTZs22rlz55U+ZAAAKo2gDysLFizQfffdpzfffFN9+vTxWjZ9+nS1bt1aDz/8sA4fPqzDhw8rJSVFf//73yVJO3fu1OHDhzV9+nRJ0rPPPqucnBzNmjVL27Zt09ChQ9WrVy/l5+d7bXfUqFGaPHmyNmzYoLCwMPXv3//KHCwAAJVQ0NwGcrstHd79Lx13lar0xGlZlqWZM2fqmWee0TvvvOO5WnIuh8Oh8PBwRUZGKikpydMeFxcnSUpISFCNGjUkScePH9eUKVP06aefqnXr1pKkevXqafXq1Xr99dfVvn17z/rjxo3zzI8YMUJdu3ZVSUmJqlev7q/DBwCg0gqKsPL1l8VatXC3jv+rVJL0/T+P6f/lLNCxn3/S6s9Xq1WrVpe9j+3bt6ukpEQdOnTwaj958qRatGjh1Xbdddd5/l27dm1JUnFxsVJTUy+7DgAAqpoqH1a+/rJYH76+tVx7nZr1dKBst6b/eabm/d+Nstlsl7Uft9stSXrvvfdUp04dr2V2u91rvlq1ap5/n93v2fUBAIC3Kj1mxe22tGrh7gqXXRWbrEHdJuv9D5dq4MCB591GeHi4ysrKyrVJ8mpPT0+X3W7X/v37Vb9+fa8pJSXFB0cDAEBwqtJXVg7v/pfn1k9FEmuk6PGuk/X6W8NVrVo1TZs2rVyfa665RuvXr9c333yj6OhoxcXFqW7durLZbFq6dKm6dOmiiIgIxcTE6Mknn9TQoUPldrvVrl07uVwurVmzRtHR0erbt68fjxQAgKqrSl9ZOe46f1A5K7FGiubO+D/Nnz9fTzzxRLnlTz75pEJDQ5Wenq5atWpp//79qlOnjl544QWNGDFCiYmJniszY8eO1XPPPafs7Gw1btxYd9xxh959912lpaX5/NgAAAgWNsuyrEAXcS6XyyWHwyGn06nY2NjL2tbBnT9p8dQvL9ivx9AWqtOw5mXtCwCAYObL7+9fqtJXVmo3qKGoGvZf7RNd067aDWpcmYIAAMAlq9JhJSTEppvvb/Crfdrd10AhIZf3JBAAAPCfKh1WJOl3LRLU6dGm5a6wRNe0q9OjTfW7FgkBqgwAAFyMKv000Fm/a5GgtOa1PG+wjYo9c+uHKyoAAJgvKMKKdOaWEINoAQCofKr8bSAAAFC5EVYAAIDRCCsAAMBohBUAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYDTCCgAAMJpfwsrBgwfVq1cvxcfHKzIyUtdff702btzoj10BAIAqLszXG/zpp5/Utm1b3Xrrrfrggw+UkJCgr7/+WjVq1PD1rgAAQBDweViZOHGiUlJSlJOT42m75pprfL0bAAAQJHx+G2jJkiXKyMjQvffeq4SEBLVo0UJz5sw5b//S0lK5XC6vCQAA4Cyfh5W9e/dq1qxZatCggZYtW6bHHntMgwYN0ptvvllh/+zsbDkcDs+UkpLi65IAAEAlZrMsy/LlBsPDw5WRkaE1a9Z42gYNGqSCggKtXbu2XP/S0lKVlpZ65l0ul1JSUuR0OhUbG+vL0gAAgJ+4XC45HA6/fH/7/MpK7dq1lZ6e7tXWuHFj7d+/v8L+drtdsbGxXhMAAMBZPg8rbdu21c6dO73adu3apbp16/p6VwAAIAj4PKwMHTpU69at0/jx47Vnzx7l5eVp9uzZGjBggK93BQAAgoDPw8qNN96oRYsWaf78+WratKnGjh2radOmKSsry9e7AgAAQcDnA2wvlz8H6AAAAP+oVANsAQAAfImwAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwGmEFAAAYjbACAACMRlgBAABGI6wAAACjEVYAAIDRCCsAAMBohBUAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYDTCCgAAMBphBQAAGI2wAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwGmEFAAAYjbACAACMRlgBAABGI6wAAACjEVYAAIDRCCsAAEiy2WxavHhxoMtABcICXQAAACY4fPiwatasGegyUAHCCgAg6J08eVJJSUl+3UdZWZlsNptCQripcak4YwCAoJOZmamBAwdq2LBhuuqqq9ShQwev20CtW7fWiBEjvNb57rvvVK1aNa1YsULSmYDz9NNPq06dOoqKitJNN92klStXevrPnTtXNWrU0NKlS5Weni673a59+/ZdqUOsUggrAICglJubq7CwMH3++ed6/fXXvZZlZWVp/vz5sizL07Zw4UIlJiaqffv2kqSHHnpIn3/+uRYsWKDNmzfr3nvvVadOnbR7927POidOnFB2drbeeOMNbdu2TQkJCVfm4KoYwgoAICjVr19fkyZNUsOGDdWoUSOvZffff78OHTqk1atXe9ry8vLUs2dPhYSE6Ouvv9b8+fP11ltv6eabb9bvfvc7Pfnkk2rXrp1ycnI865w6dUozZ85UmzZt1LBhQ0VFRV2x46tK/B5WsrOzZbPZNGTIEH/vCgCA8ypzl6ngSIHe3/u+jp48qpYtW563b61atdShQwf99a9/lSQVFRVp7dq1ysrKkiQVFhbKsixde+21io6O9kz5+fn6+uuvPdsJDw/Xdddd598DCwJ+HWBbUFCg2bNn8z8UACCgPt73sSZ8MUHfnvhWkrT3x7367vB3+njfx7q97u0VrpOVlaXBgwfr1VdfVV5enpo0aaLmzZtLktxut0JDQ7Vx40aFhoZ6rRcdHe35d0REhGw2m5+OKnj47crKsWPHlJWVpTlz5vAoGAAgYD7e97GGrRzmCSpnnTh1QsNWDtPH+z6ucL0ePXqopKREH374ofLy8tSrVy/PshYtWqisrEzFxcWqX7++1+Tvp4qCkd/CyoABA9S1a1fdfnvFiRUAAH8rc5dpwhcTZMk6b5+JX0xUmbusXHtUVJS6d++u0aNHa8eOHerZs6dn2bXXXqusrCz16dNHb7/9toqKilRQUKCJEyfq/fff98uxBDO/3AZasGCBCgsLVVBQcMG+paWlKi0t9cy7XC5/lAQACEKFxYXlrqicy5KlIyeOqLC4sMLlWVlZ6tq1q2655RalpqZ6LcvJydFLL72kJ554QgcPHlR8fLxat26tLl26+PQYINmsc5/L8oEDBw4oIyNDH330kefeXmZmpq6//npNmzatXP8xY8bohRdeKNfudDoVGxvry9IAAEHm/b3va/iq4RfsN/HmiepSj5BxOVwulxwOh1++v31+G2jjxo0qLi5Wy5YtFRYWprCwMOXn5+uVV15RWFiYysq8L7WNHDlSTqfTMx04cMDXJQEAglStyFo+7YfA8PltoNtuu01btmzxanvooYfUqFEjDR8+vNyoabvdLrvd7usyAADQDQk3KDEyUcUniisct2KTTYmRiboh4YYAVIeL5fOwEhMTo6ZNm3q1RUVFKT4+vlw7AAD+FBoSqhGtRmjYymGyyeYVWGw680jx8FbDFRoSer5NwAC8wRYAUKXdXvd2TcmcooRI71fdJ0YmakrmlPO+ZwXm8PkA28vlzwE6AIDgVeYuU2Fxob478Z1qRdbSDQk3cEXFh/z5/e3XN9gCAGCK0JBQ3Zh0Y6DLwG/AbSAAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYDTCCgAAMBphBQAAGI2wAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwGmEFAAAYjbACAACMRlgBAABGI6wAAACjEVYAAIDRCCsAAMBohBUAAGA0wgoAADAaYQUAABiNsAIAAIxGWAEAAEYjrAAAAKMRVgAAgNEIKwAAwGiEFQAAYDTCCgAAMBphBQAAGI2wAgDwuczMTA0ZMiTQZaCKIKwAAACjEVYAAIDRCCsAAL/66aef1KdPH9WsWVORkZHq3Lmzdu/e7Vn+ww8/6MEHH9TVV1+tyMhINWvWTPPnz/faRmZmpgYNGqSnn35acXFxSkpK0pgxY67wkSBQCCsAAL/q16+fNmzYoCVLlmjt2rWyLEtdunTRqVOnJEklJSVq2bKlli5dqq1bt+qRRx5R7969tX79eq/t5ObmKioqSuvXr9ekSZP04osvavny5YE4JFxhNsuyrEAXcS6XyyWHwyGn06nY2NhAlwMA+A0yMzN1/fXXa8CAAbr22mv1+eefq02bNpLOXElJSUlRbm6u7r333grX79q1qxo3bqyXX37Zs72ysjKtWrXK06dVq1b6/e9/rwkTJvj/gHBB/vz+DvPp1gAAwctdJu1bIx37VipxSpalHTt2KCwsTDfddJOnW3x8vBo2bKgdO3ZIksrKyjRhwgQtXLhQBw8eVGlpqUpLSxUVFeW1+euuu85rvnbt2iouLvb/cSHgCCsAgMu3fYn04XDJdejM/JHj0pf7ZDWIrrC7ZVmy2WySpMmTJ2vq1KmaNm2amjVrpqioKA0ZMkQnT570WqdatWpe8zabTW632/fHAuP4fMxKdna2brzxRsXExCghIUE9evTQzp07fb0bAIApti+R/q/Pf4LKWSePK33nKzp9+rTX+JMffvhBu3btUuPGjSVJq1atUvfu3dWrVy81b95c9erV8xqAC/g8rOTn52vAgAFat26dli9frtOnT6tjx446fvy4r3cFAAg0d9mZKyqqePhjg/hQdW8SrYcfflirV6/WV199pV69eqlOnTrq3r27JKl+/fpavny51qxZox07dujRRx/VkSNHruBBwHQ+vw304Ycfes3n5OQoISFBGzdu1C233OLr3QEAAmnfmvJXVLxYyulq0+DdKbrzzjt18uRJ3XLLLXr//fc9t3VGjx6toqIi3XHHHYqMjNQjjzyiHj16yOl0XpljgPH8/jTQnj171KBBA23ZskVNmzYtt/zsQKqzXC6XUlJSeBoIACqDLX+T/v7HC/f7w1+kZvf4vx5ctLNPbE2bNs0n26voaaAxY8Zo8eLF2rRp02Vt26/vWbEsS8OGDVO7du0qDCrSmTEuDofDM6WkpPizJACAL0Un+rYfUAG/hpWBAwdq8+bN5d5EeK6RI0fK6XR6pgMHDvizJACAL9VtI8UmS7Kdp4NNiq1zph/wG/ktrDz++ONasmSJVqxYoauvvvq8/ex2u2JjY70mAEAlERIqdZr475lfBpZ/z3eacKYfjHP69GkNHDhQNWrUUHx8vJ599lmdHR0yb948ZWRkKCYmRklJSerZs6fXe21Wrlwpm82mTz75RBkZGUpKSpKkX32Sq6ioSPXr19ef/vSnS3rs3OdhxbIsDRw4UG+//bY+/fRTpaWl+XoXAACTpN8l3femFFvbuz02+Ux7+l2BqQsXlJubq7CwMK1fv16vvPKKpk6dqjfeeEOSdPLkSY0dO1ZfffWVFi9erKKiIvXr16/cNkaNGqXJkydr5cqVkqQBAwZUuK+tW7eqbdu2uvfeezVr1iyFhFx8BPH500ADBgxQXl6e3nnnHcXExHgeP3M4HIqIiPD17gAAJki/S2rU9T9vsI1OPHPrhysqRilzW/qi6EcVHy2R6+dTSklJ0dSpU2Wz2dSwYUNt2bJFU6dO1cMPP6z+/ft71qtXr55eeeUVtWrVSseOHVN09H9e9jdu3Di1b99eLpdLkrR+/XqVlJSoevXqnj5r167VnXfeqZEjR+rJJ5+85Lp9HlZmzZol6cwo43Pl5ORUmMgAAFVESKiUdnOgq8B5fLj1sF54d7sOO0skSUcOuxSbkKpl246oU9MzV8Vat26tyZMnq6ysTJs3b9aYMWO0adMm/fjjj57bNvv371d6erpnu7/8GQRJKi4uVmpqqqf/7bffrpdeeklDhw79TbX75TZQRRNBBQCAwPhw62H9aV6hJ6ic9fPJMv1pXqE+3HrYq72kpEQdO3ZUdHS05s2bp4KCAi1atEiSLvgzCJK8xqPUqlVLrVq10oIFCzxXXy6VX58GAgAAgVXmtvTCu9srfMdw6aEzP4fzwrvbVea2tG7dOjVo0ED/+Mc/9P3332vChAm6+eab1ahRo9/8o5ERERFaunSpqlevrjvuuENHjx695G0QVgAAqMK+KPqx3BWVs04f/V4/fDJH+/fu0UuvzNGrr76qwYMHKzU1VeHh4Xr11Ve1d+9eLVmyRGPHjv3NNURFRem9995TWFiYOnfurGPHjl3S+oQVAACqsOKjFQcVSYpq8ntZp0/q8JvD9PLzT+vxxx/XI488olq1amnu3Ll66623lJ6ergkTJujll1++rDqio6P1wQcfyLIsdenS5ZJ+M9Dvr9u/VBW9rhcAAPw2a7/+QQ/OWXfBfvMf/i+1/l38b96PP7+/ubICAEAV1iotTrUd1X/tHcOq7aiuVmlxV7KsS0JYAQCgCgsNsen5bmceNT7PO4b1fLd0hYacL84EHmEFAIAqrlPT2prV6wYlOap7tSc5qmtWrxs871kxlc9fCgcAAMzTqWltdUhP8rzBNiHmzK0fk6+onEVYAQAgSISG2C5rEG2gcBsIAAAYjbACAACMRlgBAABGI6wAAACjEVYAAIDRCCsAAMBohBUAAGA0wgoAADAaYQUAABjNuDfYWpYl6cxPTQMAgMrh7Pf22e9xXzIurBw9elSSlJKSEuBKAADApTp69KgcDodPt2mz/BGBLoPb7dahQ4cUExMjm+3CP67kcrmUkpKiAwcOKDY29gpUaD7OSXmck/I4J+VxTsrjnJTHOSnv7DnZvn27GjZsqJAQ344yMe7KSkhIiK6++upLXi82NpY/ml/gnJTHOSmPc1Ie56Q8zkl5nJPy6tSp4/OgIjHAFgAAGI6wAgAAjFbpw4rdbtfzzz8vu90e6FKMwTkpj3NSHuekPM5JeZyT8jgn5fn7nBg3wBYAAOBclf7KCgAAqNoIKwAAwGiEFQAAYDTCCgAAMFqlDiszZ85UWlqaqlevrpYtW2rVqlWBLilgsrOzdeONNyomJkYJCQnq0aOHdu7cGeiyjJKdnS2bzaYhQ4YEupSAOnjwoHr16qX4+HhFRkbq+uuv18aNGwNdVsCcPn1azz77rNLS0hQREaF69erpxRdflNvtDnRpV8xnn32mbt26KTk5WTabTYsXL/ZablmWxowZo+TkZEVERCgzM1Pbtm0LTLFXyK+dk1OnTmn48OFq1qyZoqKilJycrD59+ujQoUOBK/gKudDfyrkeffRR2Ww2TZs27bL3W2nDysKFCzVkyBCNGjVKX375pW6++WZ17txZ+/fvD3RpAZGfn68BAwZo3bp1Wr58uU6fPq2OHTvq+PHjgS7NCAUFBZo9e7auu+66QJcSUD/99JPatm2ratWq6YMPPtD27ds1efJk1ahRI9ClBczEiRP12muvacaMGdqxY4cmTZqkP//5z3r11VcDXdoVc/z4cTVv3lwzZsyocPmkSZM0ZcoUzZgxQwUFBUpKSlKHDh08v+VWFf3aOTlx4oQKCws1evRoFRYW6u2339auXbt01113BaDSK+tCfytnLV68WOvXr1dycrJvdmxVUq1atbIee+wxr7ZGjRpZI0aMCFBFZikuLrYkWfn5+YEuJeCOHj1qNWjQwFq+fLnVvn17a/DgwYEuKWCGDx9utWvXLtBlGKVr165W//79vdruvvtuq1evXgGqKLAkWYsWLfLMu91uKykpyZowYYKnraSkxHI4HNZrr70WgAqvvF+ek4p88cUXliRr3759V6YoA5zvvPzzn/+06tSpY23dutWqW7euNXXq1MveV6W8snLy5Elt3LhRHTt29Grv2LGj1qxZE6CqzOJ0OiVJcXFxAa4k8AYMGKCuXbvq9ttvD3QpAbdkyRJlZGTo3nvvVUJCglq0aKE5c+YEuqyAateunT755BPt2rVLkvTVV19p9erV6tKlS4ArM0NRUZGOHDni9Xlrt9vVvn17Pm/P4XQ6ZbPZgvoqpXTmx4h79+6tp556Sk2aNPHZdo37IcOL8f3336usrEyJiYle7YmJiTpy5EiAqjKHZVkaNmyY2rVrp6ZNmwa6nIBasGCBCgsLVVBQEOhSjLB3717NmjVLw4YN0zPPPKMvvvhCgwYNkt1uV58+fQJdXkAMHz5cTqdTjRo1UmhoqMrKyjRu3Dg9+OCDgS7NCGc/Uyv6vN23b18gSjJOSUmJRowYoZ49ewb9DxtOnDhRYWFhGjRokE+3WynDylk2m81r3rKscm3BaODAgdq8ebNWr14d6FIC6sCBAxo8eLA++ugjVa9ePdDlGMHtdisjI0Pjx4+XJLVo0ULbtm3TrFmzgjasLFy4UPPmzVNeXp6aNGmiTZs2aciQIUpOTlbfvn0DXZ4x+Lyt2KlTp/TAAw/I7XZr5syZgS4noDZu3Kjp06ersLDQ538blfI20FVXXaXQ0NByV1GKi4vLpf9g8/jjj2vJkiVasWKFrr766kCXE1AbN25UcXGxWrZsqbCwMIWFhSk/P1+vvPKKwsLCVFZWFugSr7jatWsrPT3dq61x48ZBOzBdkp566imNGDFCDzzwgJo1a6bevXtr6NChys7ODnRpRkhKSpIkPm8rcOrUKd13330qKirS8uXLg/6qyqpVq1RcXKzU1FTPZ+6+ffv0xBNP6JprrrmsbVfKsBIeHq6WLVtq+fLlXu3Lly9XmzZtAlRVYFmWpYEDB+rtt9/Wp59+qrS0tECXFHC33XabtmzZok2bNnmmjIwMZWVladOmTQoNDQ10iVdc27Ztyz3SvmvXLtWtWzdAFQXeiRMnFBLi/VEYGhoaVI8u/5q0tDQlJSV5fd6ePHlS+fn5Qft5K/0nqOzevVsff/yx4uPjA11SwPXu3VubN2/2+sxNTk7WU089pWXLll3WtivtbaBhw4apd+/eysjIUOvWrTV79mzt379fjz32WKBLC4gBAwYoLy9P77zzjmJiYjz/L8jhcCgiIiLA1QVGTExMuTE7UVFRio+PD9qxPEOHDlWbNm00fvx43Xffffriiy80e/ZszZ49O9ClBUy3bt00btw4paamqkmTJvryyy81ZcoU9e/fP9ClXTHHjh3Tnj17PPNFRUXatGmT4uLilJqaqiFDhmj8+PFq0KCBGjRooPHjxysyMlI9e/YMYNX+9WvnJDk5Wffcc48KCwu1dOlSlZWVeT5z4+LiFB4eHqiy/e5Cfyu/DG3VqlVTUlKSGjZseHk7vuzniQLof//3f626deta4eHh1g033BDUj+lKqnDKyckJdGlGCfZHly3Lst59912radOmlt1utxo1amTNnj070CUFlMvlsgYPHmylpqZa1atXt+rVq2eNGjXKKi0tDXRpV8yKFSsq/Pzo27evZVlnHl9+/vnnraSkJMtut1u33HKLtWXLlsAW7We/dk6KiorO+5m7YsWKQJfuVxf6W/klXz26bLMsy7q8uAMAAOA/lXLMCgAACB6EFQAAYDTCCgAAMBphBQAAGI2wAgAAjEZYAQAARiOsAAAAoxFWAACA0QgrAADAaIQVAABgNMIKAAAwGmEFAAAY7f8DQNd0LzANbJsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get vectors for sample words, reduce dimensions, and plot\n",
    "test_words = ['bank', 'loan', 'river', 'coffee', 'kitten',]\n",
    "\n",
    "# pre-allocate\n",
    "test_embeddings = np.zeros((len(test_words), nlp.vocab.vectors_length), dtype='f')\n",
    "\n",
    "# retrieve embeddings\n",
    "for i, word in enumerate(test_words):\n",
    "    test_embeddings[i] = nlp.vocab[word].vector\n",
    "    \n",
    "# reduce dimensions in learned space\n",
    "X = pca.transform(test_embeddings)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(len(test_words)):\n",
    "    ax.scatter(X[i,0], X[i,1])\n",
    "    ax.text(X[i,0], X[i,1], test_words[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king -> queen similarity: 0.6108840703964233\n"
     ]
    }
   ],
   "source": [
    "# How similar are the words 'king' and 'queen' in Web text?\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"king -> queen similarity:\", cosine_similarity(\n",
    "    nlp.vocab['king'].vector.reshape(1,-1), # reshape into a 2d vector with single row\n",
    "    nlp.vocab['queen'].vector.reshape(1,-1)\n",
    ").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector math\n",
    "\n",
    "Now we ask: can we remove some part of a word's meaning, add a different part, and end up closer to a specific endpoint?\n",
    "\n",
    "Another way to think about this is to imagine the task as an analogy. For example, `man:king :: woman: ___`. Many readers would complete the analogy with `queen`. Let's see if embeddings do something similar ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king - man + woman -> queen similarity: 0.6178014278411865\n"
     ]
    }
   ],
   "source": [
    "# Vector math: king - man + woman -> closer to queen?\n",
    "king_to_queen_vector = nlp.vocab['king'].vector - nlp.vocab['man'].vector + nlp.vocab['woman'].vector\n",
    "print(\"king - man + woman -> queen similarity:\", cosine_similarity(\n",
    "    king_to_queen_vector.reshape(1,-1), \n",
    "    nlp.vocab['queen'].vector.reshape(1,-1)\n",
    ").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we've moved closer to `queen` by subtracting `man` and adding `woman`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris -> Berlin:                    0.621619701385498\n",
      "Paris - France + Germany -> Berlin: 0.8145961165428162\n"
     ]
    }
   ],
   "source": [
    "# Try the same with Paris and Berlin\n",
    "print(\"Paris -> Berlin:                   \", cosine_similarity(\n",
    "    nlp.vocab['Paris'].vector.reshape(1,-1), # reshape into a 2d vector with single row\n",
    "    nlp.vocab['Berlin'].vector.reshape(1,-1)\n",
    ").item())\n",
    "\n",
    "paris_to_berlin_vector = nlp.vocab['Paris'].vector - nlp.vocab['France'].vector + nlp.vocab['Germany'].vector\n",
    "print(\"Paris - France + Germany -> Berlin:\", cosine_similarity(\n",
    "    paris_to_berlin_vector.reshape(1,-1), \n",
    "    nlp.vocab['Berlin'].vector.reshape(1,-1)\n",
    ").item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can use vectors on these kinds of \"analogy\" tasks. How could we find the *n* most similar words to a given vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus-wide similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 780688\n",
      "Has vectors: 514092\n"
     ]
    }
   ],
   "source": [
    "# How large is our model?\n",
    "string_count = 0\n",
    "vector_count = 0\n",
    "for string in nlp.vocab.strings:\n",
    "    string_count+=1\n",
    "    if nlp.vocab[string].has_vector:\n",
    "        vector_count+=1\n",
    "print(\"Vocab size:\", string_count)\n",
    "print(\"Has vectors:\", vector_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our language model has 514,092 words with embeddings (and about 265,000 more words for which we lack embeddings; we'll ignore those).\n",
    "\n",
    "Next, we'll create a word-embedding matrix over our vocabulary, so that we can calculate distances between any pair of words. This matrix will have 514,092 rows (one for each word that has an embedding in our model) and 300 columns (the number of dimensions in the embedding representation).\n",
    "\n",
    "We initialize the output matrix with zeros, since it's much faster to update an existing numpy array than it is to append to it.\n",
    "\n",
    "Note that there's an easier way to find the most similar vectors to a taget word in spaCy, but we're showing the details for clarity ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix shape: (514092, 300)\n",
      "CPU times: user 3.04 s, sys: 243 ms, total: 3.29 s\n",
      "Wall time: 3.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Make a word-vector martrix with labels\n",
    "vector_matrix = np.zeros([vector_count,nlp.vocab.vectors_length]) # Initialize the output matrix\n",
    "counter = 0\n",
    "vocab_dict = {} # Dictionary to hold word index positions in the matrix\n",
    "vocab_list = [] # List to hold words in order\n",
    "for string in nlp.vocab.strings: # iterate over the strings in our model\n",
    "    if nlp.vocab[string].has_vector: # only want the ones with embeddings\n",
    "        vocab_dict[string] = counter # record index position of this word\n",
    "        vocab_list.append(string) # add to our list of words\n",
    "        # l2-normalize the vector and update matrix\n",
    "        vector_matrix[counter] = nlp.vocab[string].vector/nlp.vocab[string].vector_norm\n",
    "        counter+=1 # increment counter\n",
    "print(\"Matrix shape:\", vector_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be tempted to iterate over the rows in the output matrix, calculating the similarity between each one and your target vector. This is slow. Instead, we'll use matrix math.\n",
    "\n",
    "The **dot product** of two vectors is the sum of their element-wise products. If the vectors are (L2) normalized, their dot product = their cosine similarity. If you take the dot product of a (2D) matrix with a (1D) vector, the result is a (1D) vector representing the dot products of the 1D vector with each of the row vectors in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of vec1.vec2: 0.51\n"
     ]
    }
   ],
   "source": [
    "# example dot product\n",
    "vec1 = [0.5, 0.1]\n",
    "vec2 = [1.0, 0.1]\n",
    "result = 0.5*1.0 + 0.1*0.1\n",
    "print(\"Result of vec1.vec2:\", result)\n",
    "assert np.dot(vec1, vec2) == result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.329216"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aside: non-normalized vector lengths are proportional to word frequency in the training corpus\n",
    "np.sqrt(np.sum(nlp.vocab['the'].vector**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.329216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector_norm = length\n",
    "nlp.vocab['the'].vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(514092,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate similarities to 'king' via dot products\n",
    "similarities = np.dot(vector_matrix, vector_matrix[vocab_dict['king']])\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that NumPy dot products are really fast to calculate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king 1.0000000665322517\n",
      "kings 0.8352225610959328\n",
      "kingi 0.7910659723219304\n",
      "princes 0.788627132504877\n",
      "princeling 0.7839575771426479\n",
      "kingii 0.7835573077250901\n",
      "prince 0.7827693297525847\n",
      "ruler 0.7641486661200757\n",
      "princelings 0.7572183384349265\n",
      "kingship 0.7511986967121772\n"
     ]
    }
   ],
   "source": [
    "# Get 10 most-similar words to 'king'\n",
    "# argsort returns lowest to highest, so we get the last 10, then reverse\n",
    "top_n = np.argsort(similarities)[-10:][::-1] \n",
    "for i in top_n:\n",
    "    print(vocab_list[i], similarities[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king           0.849\n",
      "kings          0.7189\n",
      "princes        0.7097\n",
      "consort        0.7074\n",
      "princeling     0.7026\n",
      "monarch        0.6899\n",
      "princelings    0.6873\n",
      "princesses     0.6814\n",
      "prince         0.6563\n",
      "kingship       0.6498\n",
      "princess       0.6481\n",
      "ruler          0.645\n",
      "consorts       0.6377\n",
      "kingi          0.6362\n",
      "princedom      0.6355\n",
      "rulers         0.6347\n",
      "kingii         0.6259\n",
      "enthronement   0.6236\n",
      "monarchical    0.6206\n",
      "queen          0.6178\n"
     ]
    }
   ],
   "source": [
    "# Most similar to our king - man + woman vector\n",
    "# Also, make this prettier and with less repetition\n",
    "\n",
    "def print_most_similar(matrix, vector, vocab_list, n=20):\n",
    "    sim = np.dot(matrix, vector/np.linalg.norm(vector))\n",
    "    sorted_sims = np.argsort(sim)[-n:][::-1] #argsort return lowest to highest, so we get the last n, then reverse\n",
    "    last_word = ''\n",
    "    for i in sorted_sims:\n",
    "        current_word = vocab_list[i].lower()\n",
    "        if current_word != last_word: # filter out duplicates\n",
    "            print(f'{current_word:<15}{sim[i]:.4}')\n",
    "            last_word = current_word\n",
    "            \n",
    "print_most_similar(vector_matrix, king_to_queen_vector, vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure spacy\n",
    "\n",
    "Do the same thing, but entirely within spacy ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best = nlp.vocab.vectors.most_similar(king_to_queen_vector.reshape(1,-1), n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 7464393751932445219,  6599815902708227193,  8922038595323203056,\n",
       "         14534229509275002879,  4996998655728566540, 13490352757374322128,\n",
       "          8850213465902251097,  5501931736933901712, 12278543830867659210,\n",
       "          4661946599427795797,  8637828561746775750, 18205181805348499458,\n",
       "          8361734997143773196, 17374384407900319625,  8998572079690830222,\n",
       "         11774730061229767519,  5749989767048122345,  6761847573992876072,\n",
       "          7098074549745131472,  4176741725343376093]], dtype=uint64),\n",
       " array([[  2074,   9651,  19885,  24651, 296691,  16162, 391455,  31441,\n",
       "           7169,  52459,   8002,   9214,  57917, 452551, 211113,  12475,\n",
       "         285771, 111886,  97497,   5139]], dtype=int32),\n",
       " array([[0.849 , 0.7189, 0.7097, 0.7074, 0.7026, 0.6899, 0.6873, 0.6814,\n",
       "         0.6563, 0.6498, 0.6481, 0.645 , 0.6377, 0.6362, 0.6355, 0.6347,\n",
       "         0.6259, 0.6236, 0.6206, 0.6178]], dtype=float32))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the output\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output format of spacy's `most_similar()` function is a tuple that contains three items: an array of word hashes (unique IDs), an array of index positions in the vector matrix, and an array of similarity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king           0.849\n",
      "kings          0.7189\n",
      "princes        0.7097\n",
      "consort        0.7074\n",
      "princeling     0.7026\n",
      "monarch        0.6899\n",
      "princelings    0.6873\n",
      "princesses     0.6814\n",
      "prince         0.6563\n",
      "kingship       0.6498\n",
      "princess       0.6481\n",
      "ruler          0.645\n",
      "consorts       0.6377\n",
      "kingi          0.6362\n",
      "princedom      0.6355\n",
      "rulers         0.6347\n",
      "kingii         0.6259\n",
      "enthronement   0.6236\n",
      "monarchical    0.6206\n",
      "queen          0.6178\n"
     ]
    }
   ],
   "source": [
    "# Print top matches\n",
    "hashes, lines, scores = best\n",
    "last_word = ''\n",
    "for i in range(len(hashes[0])):\n",
    "    current_word = nlp.vocab[hashes[0][i].item()].text.lower()\n",
    "    if current_word != last_word:\n",
    "        print(f'{current_word:<15}{scores[0][i]:.4}')\n",
    "        last_word = current_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_most_similar(word, n=20):\n",
    "    target_vector = nlp.vocab[word].vector/nlp.vocab[word].vector_norm\n",
    "    hashes, lines, scores = nlp.vocab.vectors.most_similar(target_vector.reshape(1,-1), n=n)\n",
    "    last_word = ''\n",
    "    for i in range(len(hashes[0])):\n",
    "        current_word = nlp.vocab[hashes[0][i].item()].text.lower()\n",
    "        if current_word != last_word:\n",
    "            print(f'{current_word:<15}{scores[0][i]:.4}')\n",
    "            last_word = current_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "economy        1.0\n",
      "economie       0.9084\n",
      "economaidd     0.9051\n",
      "economi        0.9032\n",
      "economies      0.8936\n",
      "\n",
      "economic       1.0\n",
      "economico      0.9367\n",
      "noneconomic    0.9359\n",
      "uneconomic     0.92\n",
      "economi        0.9188\n",
      "\n",
      "money          1.0\n",
      "money-         0.9568\n",
      "money.</p      0.9499\n",
      "money--        0.9441\n",
      "money'll       0.9408\n",
      "\n",
      "wealth         1.0\n",
      "prosperity     0.7371\n",
      "bridewealth    0.7223\n",
      "wealthier      0.6775\n",
      "weal           0.6741\n",
      "\n",
      "child          1.0\n",
      "child-         0.9139\n",
      "child--        0.9041\n",
      "child(ren      0.8652\n",
      "childbed       0.8612\n",
      "\n",
      "housework      1.0\n",
      "housekeeping   0.7763\n",
      "housecleaning  0.7461\n",
      "schoolwork     0.7164\n",
      "homeschooling  0.7067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get most similar vectors to each of a list of target terms\n",
    "for term in ['economy', 'economic', 'money', 'wealth', 'child', 'housework']:\n",
    "    print_most_similar(term, n=5)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarities using word embeddings\n",
    "\n",
    "It's fun to study words, but we mostly care about documents. How can we use word embeddings to perform the kinds of document-similarity tasks that have occupied us so far this semester?\n",
    "\n",
    "One possible answer is that we might replace each word token with its corresponding embedding, then, for each document, average the embeddings to get a representation of the document. We can then use those document representations (of dimensionality equal to the dimensionality of our embedding vectors) as input to any of the methods we've already discussed.\n",
    "\n",
    "Here, we use our derived document representations to measure the cosine similarity between different (toy) documents. We do this both with and without removing stopwords, and we compare our measures to those using word counts (rather than embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc 0: The young cat is smaller than the old dog.\n",
      "Doc 1: The kitten is tinier than the elderly hound.\n",
      "   Built-in similarity: 0.891\n",
      "   Hand similarity:     0.891\n",
      "   Embed sim, no stops: 0.726\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 0: The young cat is smaller than the old dog.\n",
      "Doc 2: Not all sentences are about animals, nor about their properties.\n",
      "   Built-in similarity: 0.618\n",
      "   Hand similarity:     0.618\n",
      "   Embed sim, no stops: 0.343\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 0: The young cat is smaller than the old dog.\n",
      "Doc 3: It is a truth universally acknowledged ...\n",
      "   Built-in similarity: 0.672\n",
      "   Hand similarity:     0.672\n",
      "   Embed sim, no stops: 0.146\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 1: The kitten is tinier than the elderly hound.\n",
      "Doc 2: Not all sentences are about animals, nor about their properties.\n",
      "   Built-in similarity: 0.635\n",
      "   Hand similarity:     0.635\n",
      "   Embed sim, no stops: 0.414\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 1: The kitten is tinier than the elderly hound.\n",
      "Doc 3: It is a truth universally acknowledged ...\n",
      "   Built-in similarity: 0.712\n",
      "   Hand similarity:     0.712\n",
      "   Embed sim, no stops: 0.248\n",
      "   Token sim, no stops: 0.0\n",
      "\n",
      "Doc 2: Not all sentences are about animals, nor about their properties.\n",
      "Doc 3: It is a truth universally acknowledged ...\n",
      "   Built-in similarity: 0.52\n",
      "   Hand similarity:     0.52\n",
      "   Embed sim, no stops: 0.474\n",
      "   Token sim, no stops: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Doc similarities\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set up sample sentences\n",
    "sentences = [\n",
    "    'The young cat is smaller than the old dog.',\n",
    "    'The kitten is tinier than the elderly hound.',\n",
    "    'Not all sentences are about animals, nor about their properties.',\n",
    "    'It is a truth universally acknowledged ...'\n",
    "]\n",
    "\n",
    "# process sentences into spacy nlp objects\n",
    "docs = []\n",
    "for sentence in sentences:\n",
    "    tokens = nlp(sentence)\n",
    "    docs.append(tokens)\n",
    "    \n",
    "def remove_noninformative_tokens(doc):\n",
    "    '''\n",
    "    Takes a spacy-processed document.\n",
    "    Returns a list of spacy token objects without stopwords, punctuation, or embedding-less tokens.\n",
    "    '''\n",
    "    culled = [token for token in doc if not (token.is_stop or token.is_punct) and token.has_vector]\n",
    "    return(culled)\n",
    "\n",
    "def embedding_similarity(X,Y):\n",
    "    '''\n",
    "    Takes two lists of spacy token objects.\n",
    "    Returns cosine similarity between their embedding representations.\n",
    "    '''\n",
    "    mean_vector_X = np.mean([token.vector for token in X], axis=0)\n",
    "    mean_vector_Y = np.mean([token.vector for token in Y], axis=0)\n",
    "        # note below reshape 1D -> 2D array. 2D version is 1 row x n columns\n",
    "    cos_similarity = cosine_similarity(mean_vector_X.reshape(1, -1), mean_vector_Y.reshape(1, -1))\n",
    "    return(cos_similarity.item()) # use .item() to get single value from array\n",
    "    \n",
    "def token_similarity(X,Y):\n",
    "    '''\n",
    "    Takes two lists of spacy tokens.\n",
    "    Returns cosine similarity between their tokens.\n",
    "    '''\n",
    "    doc1 = ' '.join([token.text for token in X])\n",
    "    doc2 = ' '.join([token.text for token in Y])\n",
    "    vectorizer = TfidfVectorizer(use_idf=False)\n",
    "    vectors = vectorizer.fit_transform([doc1, doc2])\n",
    "    return(cosine_similarity(vectors[0], vectors[1]).item())\n",
    "    \n",
    "for i, doc in enumerate(docs):\n",
    "    doc1_nostops = remove_noninformative_tokens(doc)\n",
    "    for j in range(len(docs)):\n",
    "        if j>i:\n",
    "            doc2_nostops = remove_noninformative_tokens(docs[j])\n",
    "            print(f\"Doc {i}: {docs[i]}\")\n",
    "            print(f\"Doc {j}: {docs[j]}\")\n",
    "            print(f\"   Built-in similarity: {doc.similarity(docs[j]):.3}\")\n",
    "            print(f\"   Hand similarity:     {embedding_similarity(doc, docs[j]):.3}\")\n",
    "            print(f\"   Embed sim, no stops: {embedding_similarity(doc1_nostops, doc2_nostops):.3}\")\n",
    "            print(f\"   Token sim, no stops: {token_similarity(doc1_nostops, doc2_nostops):.3}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
